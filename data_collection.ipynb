{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP package used to aid in text manipulation\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Machine Learning modules used to prepare and measure text\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import trange  # Progress bar\n",
    "import pdfplumber  # PDF text extraction\n",
    "\n",
    "# HTML text processing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Helper modules\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange  # Progress bar\n",
    "\n",
    "\n",
    "import eikon as ek\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Convenient modules to simplify API access to Filings\n",
    "pd.set_option('display.max_colwidth', 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import eikon as ek\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('eikon.cfg')\n",
    "\n",
    "#Retrieve the API key\n",
    "api_key = config['Refinitiv_API']['researchNLP_API_KEY']\n",
    "\n",
    "\n",
    "ek.set_app_key('')\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-fls',num_labels=3)\n",
    "\n",
    "# Download the Pre-trained transformer used to process our raw text\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-fls')\n",
    "\n",
    "# Download the FinBert model used to process our transformed data\n",
    "nlp = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *finbert-fls* model was specifically designed to analyze financial text to provide a score for each of the predictive states defined by the model:\n",
    "* Specific FLS\n",
    "* Non-specific FLS\n",
    "* Not FLS\n",
    "\n",
    "For example, we can feed in any text into the model to demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nlp(\"The future for next years sales will increase by 10 %.\", top_k=3)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 2 outcomes (Specific and Non-specific) represent statements that are **Forward Looking Statements**.  However, the main difference is that 'specific' statements can be interpreted as more precise or specific.  For example:\n",
    "\n",
    "> *The future for next years sales will increase by 10 %.*\n",
    "\n",
    "The above statement is specific in that it is precise regarding the increase in sales by 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversely, if we change the above statement slightly...\n",
    "prediction = nlp(\"The future for next years sales will increase.\", top_k=3)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the following statement:\n",
    "\n",
    "> *The future for next years sales will increase.*\n",
    "\n",
    "leans less against specific and more towards non-specific as it is not clear what the sales increase will be.  In general, non-specific statements can be ambiguous and potentially lead to uncertainty where specific statements provide more confidence and clarity.\n",
    "\n",
    "#### Sentiment\n",
    "As part of our analysis, I plan to evaluate the Forward Looking Statement to measure the sentiment of each.  While Forward Looking Statements can provide predictions about future business conditions, these conditions are not necessarily positive.  It might be useful to measure the overall tone of the forward looking statements during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment - Download the Pre-trained transformer used to process our raw text\n",
    "sent_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# Sentiment - Download the FinBert model used to process our transformed data\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF Text Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or ''  # Ensuring that None is not returned\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove file paths and document headers\n",
    "    text = re.sub(r'Filings/[\\w\\.-]+', '', text)\n",
    "    \n",
    "    # Remove malformed characters and symbols\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r'[\\r\\n]+', '\\n', text)  # Normalize new lines\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove citation-like numbers in brackets\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)  # Replace multiple spaces with a single space\n",
    "\n",
    "    # Optional: Remove numerical data that's irrelevant (careful with this, as financial data is often numerical)\n",
    "    text = re.sub(r'\\b\\d+\\.\\d+\\b', '', text)  # Remove floating numbers (if they are not relevant)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "with open('Filings copy.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(cleaned_text)  # Print first 500 characters to check the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filings\n",
    "The Filings service available within Refinitiv provides the ability to search and extract Global Filings documents for thousands of companies spanning over 50 years of history.  As part of the service, the interface supports the extraction of specific sections within a document.  For example, 10-Q filings are well-structured documents containing specific sections such as Management Discussion, Exhibits, Cashflow Statements, etc.  By utilizing the capabilities offered within the Filings GraphQL interface, I can explicitly choose a specific section for analysis.  Because the <b>entire Filings document</b> is typically complex and contains many standard sections, this will introduce too much insignificant text that may skew the analysis.  Instead, our goal is to utilize the power of the service to choose a specific section that can offer text that may dictate the tone representative of the overall sentiment of the Filing.\n",
    "#### Define the input properties required for analysis\n",
    "There are a number of ways I can collect filings data and the strategy I can use to evaluate the text.  For this analysis, I've chosen to use a single company and capture multiple reports over time.  One thing I considered was that I intentionally avoided mixing filings from multiple companies.  It's quite possible that we may get more accurate results if we focus on the writing style and tone from the same company.\n",
    "\n",
    "To begin, I'll prepare criteria that acts as the basis for the data sets by choosing a company and a time frame to capture content.\n",
    "### Filings text extraction\n",
    "The above request performed the query to retrieve quarterly SEC filings for the company Tesla.  Within the response contains meta data related to the filings as well as the key information requested - the text containing the \"Management Discussion\".  At this point, I plan to prepare for the next step of my analysis by extracting this text for each filings document reported.\n",
    "#### Capture Closing Prices\n",
    "As part of the analysis, I want to capture the closing price based on the Filings date.  The goal is to look for any possible correlation with the intelligence we extract from the text and the performance of the stock.  While we know performance is based on many factors, the point of this measure is to evaluate one of many possible trends as a way to look for correlation.\n",
    "### AI model evaluation\n",
    "For each of the above filings, break down the entire \"Management Discussions and Analysis\" segment into individual sentences and run each through the pretrained FinBert models to generate our predictions.  The *evaluate()* function below performs the following analysis for each filings report:\n",
    "\n",
    "For each sentence:\n",
    "* Evaluate the FLS predictions\n",
    "* Evaluate the Sentiment\n",
    "\n",
    "Collect the above FLS results to measure the percentage of sentences that are Forward-looking statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text):\n",
    "    # Initialize containers for the results\n",
    "    fls_pct = 0\n",
    "    total_sentiments = torch.zeros(1, 3)\n",
    "\n",
    "    # Tokenize the text into individual sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    fls_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # FLS prediction\n",
    "        prediction = nlp(sentence[:512], top_k=3)[0]['label']\n",
    "\n",
    "        # Check if the sentence is a Forward Looking Statement\n",
    "        if prediction.startswith(\"Specific\") or prediction.startswith(\"Non\"):\n",
    "            fls_count += 1\n",
    "\n",
    "            # Tokenize for sentiment analysis\n",
    "            encoded_input = sent_tokenizer(sentence, return_tensors=\"pt\", truncation=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Sentiment prediction\n",
    "                output = model(**encoded_input)\n",
    "                sentiment = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "                total_sentiments += sentiment\n",
    "\n",
    "    # Calculate the percentage of FLS sentences\n",
    "    if num_sentences > 0:\n",
    "        fls_pct = (fls_count / num_sentences) * 100\n",
    "        average_sentiments = total_sentiments / fls_count  # Average the sentiments\n",
    "        dominant_sentiment_index = average_sentiments.argmax(dim=1).item()\n",
    "        dominant_sentiment = model.config.id2label[dominant_sentiment_index]\n",
    "        sentiment_confidence = average_sentiments[0][dominant_sentiment_index].item() * 100\n",
    "    else:\n",
    "        dominant_sentiment = 'neutral'\n",
    "        sentiment_confidence = 0\n",
    "\n",
    "\n",
    "    return fls_pct, dominant_sentiment, sentiment_confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Workflow\n",
    "This is where we integrate the PDF processing into our existing workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_relevant_text(text):\n",
    "    # List of keywords that might indicate forward-looking statements\n",
    "    keywords = ['future', 'forecast', 'project', 'estimate', 'expect', 'anticipate']\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    relevant_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if any(keyword in sentence.lower() for keyword in keywords):\n",
    "            relevant_sentences.append(sentence)\n",
    "\n",
    "    return ' '.join(relevant_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = ek.get_timeseries(\"INVESTB.AD\", start_date=\"2000-01-01\", end_date=None , interval=\"daily\", fields=\"CLOSE\")\n",
    "\n",
    "# fill in all missing dates with the previous day's value\n",
    "timeseries = timeseries.asfreq('D', method='pad')\n",
    "timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_date_str = \"2008-09-30\"\n",
    "\n",
    "\n",
    "stock_price = timeseries.loc[formatted_date_str]['CLOSE']\n",
    "print(stock_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanitize_folder_name(name):\n",
    "    \"\"\" Sanitize the folder name by stripping leading/trailing spaces and replacing problematic characters. \"\"\"\n",
    "    return name.strip().replace(':', '').replace('\\\\', '').replace('/', '').replace('*', '').replace('?', '').replace('\"', '').replace('<', '').replace('>', '').replace('|', '')\n",
    "\n",
    "i = 0\n",
    "divisions = []\n",
    "base_path = \"Filings\"\n",
    "\n",
    "for f in os.listdir(base_path):\n",
    "    print(f) \n",
    "    i += 1\n",
    "    division = f.split(\" - \")\n",
    "    divisions.append(division)\n",
    "\n",
    "    if len(division) < 5:\n",
    "        print(f\"Skipping file {f} as it doesn't have enough parts when split.\")\n",
    "        continue\n",
    "    sanitized_folder_name = sanitize_folder_name(division[2])\n",
    "    folder_path = os.path.join(base_path, sanitized_folder_name)\n",
    "\n",
    "    # Check and create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created.\")\n",
    "\n",
    "    # Define the current file path\n",
    "    current_file_path = os.path.join(base_path, f)\n",
    "\n",
    "    # Define the new file path in the destination folder\n",
    "    new_file_path = os.path.join(folder_path, f)\n",
    "\n",
    "    # If the file already exists at the destination, delete it\n",
    "    if os.path.exists(new_file_path):\n",
    "        os.remove(new_file_path)\n",
    "        print(f\"Existing file '{new_file_path}' has been deleted.\")\n",
    "\n",
    "    # Move the file to the new folder\n",
    "    try:\n",
    "        os.rename(current_file_path, new_file_path)\n",
    "        print(f\"File '{f}' moved to '{folder_path}'.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error moving file {f}: {e}\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(divisions)\n",
    "\n",
    "\n",
    "df.columns = ['desc', 'date', 'ticker', 'full name', 'type', 'filing', 'extension', 'dk']\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "@contextmanager\n",
    "def get_cursor():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            dbname=\"neuralfin-gpt\",\n",
    "            user=\"postgres\",\n",
    "            password=\"1Y9k5t0mniR\"\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        yield cursor\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "with get_cursor() as cursor:\n",
    "    # Execute a query\n",
    "    cursor.execute(\"SELECT * FROM companies;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM companies;\")\n",
    "\n",
    "companies = cursor.fetchall()\n",
    "\n",
    "for company in companies:\n",
    "  print(company)\n",
    "  \n",
    "cursor.execute(\"SELECT * FROM reports_metadata;\")\n",
    "\n",
    "reports = cursor.fetchall()\n",
    "\n",
    "for report in reports:\n",
    "  print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save text to a file\n",
    "def save_text_to_file(folder_path, ticker, date, text):\n",
    "\n",
    "    filename = f\"{ticker}_{date}.txt\"\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    return file_path\n",
    "\n",
    "# Function to check and insert into the companies table\n",
    "def check_and_insert_company(company_name, ticker):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\"SELECT company_id FROM companies WHERE ticker = %s\", (ticker,))\n",
    "        result = cursor.fetchone()\n",
    "        if result:\n",
    "            return result[0]\n",
    "        else:\n",
    "            cursor.execute(\"INSERT INTO companies (company_name, ticker) VALUES (%s, %s) RETURNING company_id;\", (company_name, ticker))\n",
    "            return cursor.fetchone()[0]\n",
    "\n",
    "# Function to insert into the contents table and return content ID\n",
    "def insert_into_contents(file_path):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\"INSERT INTO contents (file_path) VALUES (%s) RETURNING content_id;\", (file_path,))\n",
    "        return cursor.fetchone()[0] \n",
    "\n",
    "# Function to insert data into the metadata table\n",
    "def insert_data_into_db (data):\n",
    "    with get_cursor() as cursor:\n",
    "        cursor.execute(\"\"\"\n",
    "        INSERT INTO reports_metadata (company_id, date, doc_type, sentiment_perc, content_id, sentiment_lit, fls_pct) \n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s);\n",
    "        \"\"\", (data['Company ID'], data['Date'], data['Doc Type'], data['Sentiment Confidence'], data['Content ID'], data['Sentiment'], data['FLS_Percentage']))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "def check_if_report_exists(company_id, document_date):\n",
    "    with get_cursor() as cursor:\n",
    "        # Query to check the database\n",
    "        query = \"SELECT EXISTS(SELECT 1 FROM reports_metadata WHERE company_id=%s AND date=%s)\"\n",
    "        cursor.execute(query, (company_id, document_date))\n",
    "        return cursor.fetchone()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will process all the PDF files in the folders and extract the relevant information, then save it to the database\n",
    "folder_path = \"MENA_Filings-v1\"\n",
    "data_per_comp = []\n",
    "\n",
    "#for folder_path in folders:\n",
    "# List all files in the folder and filter for PDF files\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        print(f\"Processing {full_path}\")\n",
    "        full_path = os.path.join(folder_path, pdf_file)\n",
    "        text = extract_text_from_pdf(full_path)\n",
    "        text = clean_text(text)\n",
    "        \n",
    "\n",
    "        parts = pdf_file.split(\" - \")\n",
    "\n",
    "        #extract the required information from the file name\n",
    "        document_date = parts[1]\n",
    "        ticker_symbol = parts[2]\n",
    "        document_type = parts[4]\n",
    "        company_name = parts[3]\n",
    "\n",
    "        \n",
    "        \n",
    "        # Parse the date string into a datetime object\n",
    "        date_obj = datetime.strptime(document_date, '%Y-%b-%d')\n",
    "        formatted_date_str = date_obj.strftime('%Y-%m-%d')\n",
    "        \n",
    "        \n",
    "        text_file_path = save_text_to_file('filings-txt-v1', ticker_symbol, formatted_date_str, text)\n",
    "        content_id = insert_into_contents(text_file_path)\n",
    "        company_id = check_and_insert_company(company_name, ticker_symbol)\n",
    "\n",
    "\n",
    "        if check_if_report_exists(company_id, formatted_date_str):\n",
    "            print(f\"Report for {ticker_symbol} on {formatted_date_str} already exists in the database. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        fls_pct, dominant_sentiment, sentiment_confidence = evaluate(text)\n",
    "        \n",
    "\n",
    "\n",
    "        row_data = {\n",
    "            'Company ID': company_id,\n",
    "            'Date': formatted_date_str,\n",
    "            'Doc Type': document_type,\n",
    "            'Sentiment': dominant_sentiment,\n",
    "            'Content ID': content_id,\n",
    "            'FLS_Percentage': fls_pct,\n",
    "            'Sentiment Confidence': sentiment_confidence\n",
    "        }\n",
    "        \n",
    "        print(row_data)\n",
    "\n",
    "        insert_data_into_db(row_data)\n",
    "        data_per_comp.append(row_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {pdf_file}, Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_per_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data_per_comp, columns=['Ticker', 'Company Name', 'Date', 'Doc Type', 'Sentiment', 'FLS_Percentage'])\n",
    "results.to_csv('CSVs/test1.csv', index = False)\n",
    "\n",
    "sorted_result = results.sort_values(by=['Date'], ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the database\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        dbname=\"neuralfin-gpt\",\n",
    "        user=\"postgres\",\n",
    "        password=\"1Y9k5t0mniR\"\n",
    "    )\n",
    "\n",
    "# Execute the query and fetch data as a DataFrame\n",
    "with get_connection() as conn:\n",
    "    query = \"\"\"\n",
    "    SELECT B.file_path, A.sentiment_perc, A.sentiment_lit, A.fls_pct \n",
    "    FROM reports_metadata AS A \n",
    "    INNER JOIN contents AS B ON A.content_id = B.content_id \n",
    "    WHERE fls_pct > 0;\n",
    "    \"\"\"\n",
    "    data = pd.read_sql(query, conn)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Content from Files\n",
    "def read_file_content(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "data['content'] = data['file_path'].apply(read_file_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the Dataset\n",
    "\n",
    "dataset = [\n",
    "    {\n",
    "        \"instruction\": \"Analyze the sentiment of the following financial report.\",\n",
    "        \"input\": row[\"content\"],\n",
    "        \"output\": row[\"sentiment_lit\"],\n",
    "        \"fls_pct\": row[\"fls_pct\"],\n",
    "        \"sentiment_confidence\": row[\"sentiment_perc\"]\n",
    "    }\n",
    "    for _, row in data.iterrows()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(dataset)\n",
    "dataset.to_parquet(\"NeuralFin-GPT_MENA_Financial_Sentiments.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
